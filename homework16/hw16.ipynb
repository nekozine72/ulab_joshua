{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64ea8b9-97fa-4a44-b952-7bdff856f76c",
   "metadata": {},
   "source": [
    "# HW 16 - Animation\n",
    "ULAB - Physics and Astronomy Division \\\n",
    "Due **Sunday, March 16th, 2025 at 11:59pm** on Gradescope\n",
    "\n",
    "**Make sure to work on this notebook in DataHub!**\n",
    "\n",
    "# 1) Recap of lecture\n",
    "**Copy and paste** your code from lecture for the neural network with the **default** settings (i.e. before we improved the model). Make sure to also import the `data.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "54073aa9-721e-4728-89d4-0225e2eb442c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\joshj\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\joshj\\anaconda3\\lib\\site-packages (0.21.0+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\joshj\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\joshj\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\joshj\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\joshj\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joshj\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\joshj\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\joshj\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\joshj\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\joshj\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\joshj\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\joshj\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joshj\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8a92d212-984e-4aa1-8d8e-90695822b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import matplotlib.pyplot as plt\n",
    "import torch # This is PyTorch\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 50), # Input layer -> Hidden layer\n",
    "    torch.nn.Sigmoid(), # Activation function\n",
    "    torch.nn.Linear(50, 1) # Hidden layer -> Output layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af24faec-4324-494c-b38a-4f202597c778",
   "metadata": {},
   "source": [
    "What is the difference between artificial intelligence, machine learning and deep learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8f65992b-8480-4e5f-b776-8f12dfa3af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI is the biggest term, meaning anything that mimics human intelligence\n",
    "#Machine Learning is a subset of AI, where a computer learns from given data\n",
    "#Deep learning is a subset of machine learning that uses neural networks to solve complex problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec2cdf-8c44-4ceb-8c71-31e404b334c6",
   "metadata": {},
   "source": [
    "What is a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d629e86d-bb56-47aa-bea3-3c52c01e47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A neural network is a learning model that recognizes patterns using nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c6ce7-9c55-4d51-af1b-ebd44efb41f0",
   "metadata": {},
   "source": [
    "In lecture, we used `torch.nn.Linear(2, 50)`. What do the numbers `2` and `50` represent? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "570cf33a-3e77-4d6f-aaa6-d57b925f94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#They represent the number of input and output features respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22255b9-875d-4d55-9b81-8a7ddcff63ff",
   "metadata": {},
   "source": [
    "What does the activation function do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4f8ae436-854e-4da0-babe-e20ec60b33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It allows neural networks to learn more complex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaaf982-6605-45db-8dad-4b42124793e1",
   "metadata": {},
   "source": [
    "What is a loss function? Why do we need one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "949bcb88-9cb6-402b-9b30-048ac3b82347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It calculates how much a prediction matches the actual values. It's a useful way to tell how accurate a model is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb58806-7764-447f-bdc6-8ca26bddc2df",
   "metadata": {},
   "source": [
    "Why do we call `optimizer.zero_grad()` before calling `loss.backward()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "15669992-bfe6-4cc4-b646-eb4ce9516456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So that we can zero or reset the parameters in the model before computing new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00058421-3ebe-4089-b501-6a792a0f55dc",
   "metadata": {},
   "source": [
    "# 2) Modifying the Neural Network\n",
    "What line do you need to modify to use **30 neurons** instead of **50**? Don't change anything else. Give the original and then modified line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b4a437c1-9985-47a9-800f-70c68c5991c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 30), # Input layer -> Hidden layer\n",
    "    torch.nn.Sigmoid(), # Activation function\n",
    "    torch.nn.Linear(30, 1) # Hidden layer -> Output layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d0d30-0f73-4e8b-bc29-9baa082c85c7",
   "metadata": {},
   "source": [
    "With your default code above, run your model with **30 neurons**. Compare the loss functions from lecture to your modification on the **1000th** iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "91b9a1a9-ee3c-45e8-98ca-7d0acb7ce063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0843770503997803\n",
      "Iteration 100, Loss: 0.9927799701690674\n",
      "Iteration 200, Loss: 0.9296799898147583\n",
      "Iteration 300, Loss: 0.8333325386047363\n",
      "Iteration 400, Loss: 0.7238421440124512\n",
      "Iteration 500, Loss: 0.6180348992347717\n",
      "Iteration 600, Loss: 0.5285981297492981\n",
      "Iteration 700, Loss: 0.4594351649284363\n",
      "Iteration 800, Loss: 0.4032338261604309\n",
      "Iteration 900, Loss: 0.35947638750076294\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(1000):\n",
    "    y_pred = model(data.x)            \n",
    "    loss = loss_fn(y_pred, data.y)    \n",
    "    optimizer.zero_grad()             \n",
    "    loss.backward()                   \n",
    "    optimizer.step()    \n",
    "\n",
    "    if t % 100 == 0:\n",
    "        print(f\"Iteration {t}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ff2721a8-5ae0-486f-a0ed-0956d18ae1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is much more loss than in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93701af6-bf34-4903-a267-7f29e7c06cee",
   "metadata": {},
   "source": [
    "Return to **50 neurons**. Change your activation function from `torch.nn.Sigmoid()` to `torch.nn.ReLU`. Compare the loss functions from lecture to your modification on the **1000th** iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0130ffcf-3622-4275-b7aa-b9c116bd8962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.151300072669983\n",
      "Iteration 100, Loss: 1.00050950050354\n",
      "Iteration 200, Loss: 0.9562192559242249\n",
      "Iteration 300, Loss: 0.8695255517959595\n",
      "Iteration 400, Loss: 0.7395151853561401\n",
      "Iteration 500, Loss: 0.6098431348800659\n",
      "Iteration 600, Loss: 0.513172447681427\n",
      "Iteration 700, Loss: 0.4487796425819397\n",
      "Iteration 800, Loss: 0.40654534101486206\n",
      "Iteration 900, Loss: 0.378874808549881\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 50), # Input layer -> Hidden layer\n",
    "    torch.nn.Sigmoid(), # Activation function\n",
    "    torch.nn.Linear(50, 1) # Hidden layer -> Output layer\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(1000):\n",
    "    y_pred = model(data.x)            \n",
    "    loss = loss_fn(y_pred, data.y)    \n",
    "    optimizer.zero_grad()             \n",
    "    loss.backward()                   \n",
    "    optimizer.step()    \n",
    "\n",
    "    if t % 100 == 0:\n",
    "        print(f\"Iteration {t}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "20e2c995-5936-47f8-827b-2b50991477d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is less less than the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc32f30-c05a-4c79-ae9c-fe7909668bef",
   "metadata": {},
   "source": [
    "Change your activation function from `torch.nn.Sigmoid()` to `torch.nn.Tanh`? Compare the loss functions from lecture to your modification on the **1000th** iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cf319a16-d707-4c80-a326-d7248ffec8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.421966552734375\n",
      "Iteration 100, Loss: 0.8429650664329529\n",
      "Iteration 200, Loss: 0.6509761214256287\n",
      "Iteration 300, Loss: 0.48537155985832214\n",
      "Iteration 400, Loss: 0.38690152764320374\n",
      "Iteration 500, Loss: 0.32851067185401917\n",
      "Iteration 600, Loss: 0.2914893925189972\n",
      "Iteration 700, Loss: 0.26410582661628723\n",
      "Iteration 800, Loss: 0.2419167459011078\n",
      "Iteration 900, Loss: 0.22257985174655914\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 50), # Input layer -> Hidden layer\n",
    "    torch.nn.Tanh(), # Activation function\n",
    "    torch.nn.Linear(50, 1) # Hidden layer -> Output layer\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(1000):\n",
    "    y_pred = model(data.x)            \n",
    "    loss = loss_fn(y_pred, data.y)    \n",
    "    optimizer.zero_grad()             \n",
    "    loss.backward()                   \n",
    "    optimizer.step()    \n",
    "\n",
    "    if t % 100 == 0:\n",
    "        print(f\"Iteration {t}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "75f447cd-7d89-4a73-ba8b-3cdda4f674cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The loss is even smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b14d48-603a-4463-b710-bf4a886e980a",
   "metadata": {},
   "source": [
    "What is the difference between `torch.nn.Sigmoid()`, `torch.nn.ReLU` and `torch.nn.Tanh`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8d3c5a56-5108-439b-a8a6-110d703ddd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid puts all values in between 0 and 1, ReLU sets negatives to 0, and Tanh is similar to sigmoid but using -1 and 1 instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d3b23-ddac-4499-ab68-f2172df71379",
   "metadata": {},
   "source": [
    "Return to `torch.nn.Sigmoid()`. Change your learning rate from `1e-3` to `1e-2` in your optimizer. Compare the loss functions from lecture to your modification on the **1000th** iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6dd3f16b-d949-4eb1-8cd6-cc2087742f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0719181299209595\n",
      "Iteration 100, Loss: 0.361671507358551\n",
      "Iteration 200, Loss: 0.2772327661514282\n",
      "Iteration 300, Loss: 0.22400139272212982\n",
      "Iteration 400, Loss: 0.19515782594680786\n",
      "Iteration 500, Loss: 0.14231516420841217\n",
      "Iteration 600, Loss: 0.0851956456899643\n",
      "Iteration 700, Loss: 0.06076016277074814\n",
      "Iteration 800, Loss: 0.0477730929851532\n",
      "Iteration 900, Loss: 0.037824906408786774\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 50), # Input layer -> Hidden layer\n",
    "    torch.nn.Sigmoid(), # Activation function\n",
    "    torch.nn.Linear(50, 1) # Hidden layer -> Output layer\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "for t in range(1000):\n",
    "    y_pred = model(data.x)            \n",
    "    loss = loss_fn(y_pred, data.y)    \n",
    "    optimizer.zero_grad()             \n",
    "    loss.backward()                   \n",
    "    optimizer.step()    \n",
    "\n",
    "    if t % 100 == 0:\n",
    "        print(f\"Iteration {t}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f307e0-a911-46c9-9abb-0715723901a2",
   "metadata": {},
   "source": [
    "Modify your model to have **two hidden layers** instead of one. Give that code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "000ef876-a2bc-47bf-a2da-694598ce26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The loss gets much smaller much quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c020d-e162-4b7e-a5d3-88ade786c2f4",
   "metadata": {},
   "source": [
    "# 3) More Advanced\n",
    "Create a new file called `new_data.py`, make a new \"hidden\" function and add noise/ obscure the data (e.g. like what is done in `data.py` from lecture). Import that file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "50bdfc18-d57a-4120-a7df-49e1e31a4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_data.py\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "import new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d96e8-474a-4be3-87aa-aa97fc478042",
   "metadata": {},
   "source": [
    "Define a new model and train it **poorly** in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b522ec91-7f34-4648-adfc-7f422dfd6fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.4319824278354645\n",
      "Iteration 100, Loss: nan\n",
      "Iteration 200, Loss: nan\n",
      "Iteration 300, Loss: nan\n",
      "Iteration 400, Loss: nan\n",
      "Iteration 500, Loss: nan\n",
      "Iteration 600, Loss: nan\n",
      "Iteration 700, Loss: nan\n",
      "Iteration 800, Loss: nan\n",
      "Iteration 900, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "def add_noise(data, noise_factor=0.1):\n",
    "    noise = noise_factor * torch.randn_like(data)\n",
    "    return data + noise\n",
    "def get_noisy_data():\n",
    "    data = torch.rand(1000, 2)\n",
    "    return add_noise(data)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "bad_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 10), # Input layer -> Hidden layer\n",
    "    torch.nn.ReLU(), # Activation function\n",
    "    torch.nn.Linear(10, 1) # Hidden layer -> Output layer\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(bad_model.parameters(), lr=10)\n",
    "data = get_noisy_data()\n",
    "targets = torch.rand(1000, 1)\n",
    "\n",
    "for t in range(1000):\n",
    "    bad_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = bad_model(data)\n",
    "    loss = loss_fn(predictions, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        print(f\"Iteration {t}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d51f6-9dae-40b5-aaa6-59965de12f9e",
   "metadata": {},
   "source": [
    "Improve your model and train it **well** in the cell below. (You can add more neurons, iterations, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3416981b-36b1-43a6-b9fd-1649dc9f0117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.3969116806983948\n",
      "Iteration 100, Loss: 0.3969116806983948\n",
      "Iteration 200, Loss: 0.3969116806983948\n",
      "Iteration 300, Loss: 0.3969116806983948\n",
      "Iteration 400, Loss: 0.3969116806983948\n",
      "Iteration 500, Loss: 0.3969116806983948\n",
      "Iteration 600, Loss: 0.3969116806983948\n",
      "Iteration 700, Loss: 0.3969116806983948\n",
      "Iteration 800, Loss: 0.3969116806983948\n",
      "Iteration 900, Loss: 0.3969116806983948\n"
     ]
    }
   ],
   "source": [
    "bad_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 50), # Input layer -> Hidden layer\n",
    "    torch.nn.ReLU(), # Activation function\n",
    "    torch.nn.Linear(50, 1) # Hidden layer -> Output layer\n",
    ")\n",
    "for t in range(1000):\n",
    "    bad_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = bad_model(data)\n",
    "    loss = loss_fn(predictions, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        print(f\"Iteration {t}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188ba64-615c-4f1b-afcb-d143c4177a76",
   "metadata": {},
   "source": [
    "Make foure subplots (two rows, two columns). \n",
    "- (TOP LEFT) Show what the \"hidden pattern\" is supposed to be.\n",
    "- (TOP RIGHT) Show that your \"data\" is.\n",
    "- (BOTTOM LEFT) Show your model's prediction when you trained it poorly.\n",
    "- (BOTTOM RIGHT) Show your model's prediction when you trained it well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "209c213f-79b1-4345-b779-f1a8c2f1baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9f4e7-4e78-436f-a7b1-5bb9dbeabbb2",
   "metadata": {},
   "source": [
    "# 4) Challenge\n",
    "Find some data online and build a neural network around it. Make sure to also answer these questions:\n",
    "- Where did you find your data? \n",
    "- How well does your model match your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8782637-11a2-485b-9501-bceebc1e3f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492582c4-529e-45a0-874b-698c5004c790",
   "metadata": {},
   "source": [
    "Once you are done, make sure to:\n",
    "- Download this notebook and all releveant files (`new_data.py`, stuff from question 4, etc) from datahub.\n",
    "- Move it to your **remote repository**.\n",
    "- Upload to Gradescope!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
